\documentclass[a4paper,12pt]{article}
\usepackage[left=1.5cm,right=1.5cm,top=2cm,bottom=2cm]{geometry}
\usepackage{cmap}
\usepackage{mathtext}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

\usepackage{graphicx}
%\graphicspath{{noiseimages/}}
\usepackage{wrapfig}
\usepackage{tabularx}
\usepackage{hyphenat}
\usepackage{hyperref}
\usepackage{gensymb}
\usepackage[rgb]{xcolor}
\hypersetup{
colorlinks=true,urlcolor=blue
}

\usepackage{adjustbox}

%%% Дополнительная работа с математикой
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools} % AMS
\usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление

%% Номера формул
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.

%% Шрифты
\usepackage{euscript}	 % Шрифт Евклид
\usepackage{mathrsfs} % Красивый матшрифт

%% Свои команды
\DeclareMathOperator{\sgn}{\mathop{sgn}}

\usepackage{graphics}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{siunitx} % Required for alignment
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{afterpage}
\usepackage[T1,T2A]{fontenc}
\usepackage{caption}
\usepackage[arrowdel]{physics}
\usepackage{booktabs}

\newcommand{\rref}[1]{(\ref{#1})}
\newcommand{\Equip}[3]{{\bf #1:} $\Delta = \pm #2$ \si{#3}

}
\newcommand{\equip}[1]{{\bf #1}

}

\begin{document}

\section{Аннотация}

В данной работе решается одна выбранная СЛАУ размерности $n=12$ несколькими прямыми и итерационными методами. Для итерационных методов строится зависимость нормы невязки от номера итерации.

\section{Методология}

Используется следующая СЛАУ (пункт ``у'' из предложенного для лабораторной набора):

\begin{equation}
\begin{gathered}
n=12, a_{i i}=1, a_{i j}=1 /\left(i^2+j\right)(i \neq j), f_i=1 / i, \\
\left\{\begin{array}{cccc}
a_{11} x_1+a_{12} x_2+\ldots+a_{1 n} x_n & = & f_1 \\
\cdots & \cdots & \cdots \\
a_{n 1} x_1+a_{n 2} x_2+\ldots+a_{n n} x_n & = & f_n
\end{array}\right.
\end{gathered}
\label{eq:sys}
\end{equation}

Использованные методы приведены ниже. Алгоритмы реализованы на языке программирования Python (см. файл my\_methods.py). Для проверки правильности реализаций написаны генератор тестовых систем и скрипт для проверки (gen\_rand\_tests.py и checker.py).

Для системы \eqref{eq:sys} проверяется, что все использованные методы дают одно и то же решение (с точностью до $\epsilon = 10^{-7}$). Для итерационных методов строится график зависимости евклидовой нормы невязки от номера итерации.

\textbf{Примечание:} СЛАУ \eqref{eq:sys} имеет несимметричную матрицу. Формально из-за этого некоторые из приведённых ниже методов не могут работать с такой системой; на практике это проверяется в исследовании. Кроме того, у данной матрицы отсутствует строгое диагональное преобладание, что является достаточным условием сходимости для некоторых методов (но не является необходимым).

\subsection*{Метод Гаусса с выбором главного элемента}

\textbf{Тип метода:} прямой.

\textbf{Условия применимости:}
\begin{enumerate}
    \item Матрица $A$ должна быть квадратной и невырожденной
\end{enumerate}

\textbf{Суть метода:}
\begin{enumerate}
    \item На каждом шаге $k$ (где $k = 0, 1, \dots, n-1$) выбирается «главный элемент» — максимальный по модулю элемент в столбце $k$ среди строк $k, k+1, \dots, n-1$
    \item Строка с главным элементом меняется местами с текущей строкой $k$
    \item Выполняется исключение элементов столбца $k$ в строках $k+1, \dots, n-1$ по формулам:
        \begin{align*}
            m_{ik} &= \frac{a_{ik}}{a_{kk}}, \quad i = k+1, \dots, n-1 \\
            a_{ij} &= a_{ij} - m_{ik} \cdot a_{kj}, \quad j = k, \dots, n-1 \\
            b_i &= b_i - m_{ik} \cdot b_k
        \end{align*}
    \item После приведения матрицы к верхнетреугольному виду выполняется обратный ход:
        \begin{align*}
            x_{n-1} &= \frac{b_{n-1}}{a_{n-1,n-1}} \\
            x_k &= \frac{b_k - \sum_{j=k+1}^{n-1} a_{kj} x_j}{a_{kk}}, \quad k = n-2, \dots, 0
        \end{align*}
\end{enumerate}

\subsection*{Метод LU-разложения}

\textbf{Тип метода:} прямой.

\textbf{Условия применимости:}
\begin{enumerate}
    \item Матрица $A$ должна быть квадратной
    \item Все ведущие главные миноры матрицы $A$ должны быть ненулевыми
    \item Матрица $A$ должна быть невырожденной
\end{enumerate}

\textbf{Суть метода:}
Метод основан на разложении матрицы системы $A$ на произведение двух треугольных матриц:
\[ A = LU \]
где:
\begin{itemize}
    \item $L$ — нижняя треугольная матрица (с единичной диагональю)
    \item $U$ — верхняя треугольная матрица
\end{itemize}
Решение системы $Ax = b$ сводится к последовательному решению двух систем с треугольными матрицами:
\[
\begin{cases}
Ly = b \\
Ux = y
\end{cases}
\]

\textbf{Алгоритм:}
\begin{enumerate}
    \item Выполнить LU-разложение матрицы $A$
    \item Решить систему $Ly = b$ прямой подстановкой
    \item Решить систему $Ux = y$ обратной подстановкой
\end{enumerate}

\subsection*{Метод Якоби}

\textbf{Тип метода:} итерационный.

\textbf{Условия применимости:}
\begin{enumerate}
    \item Матрица $A$ должна иметь ненулевые диагональные элементы ($a_{ii} \neq 0$)
    \item Достаточное условие сходимости: строгое диагональное преобладание ($|a_{ii}| > \sum_{j \neq i} |a_{ij}|$)
\end{enumerate}

\textbf{Суть метода:}
\begin{enumerate}
    \item Исходная система $Ax = b$ преобразуется к виду $x = Bx + c$
    \item Матрица $B$ и вектор $c$ вычисляются как:
        \begin{align*}
            B_{ij} &= -\frac{a_{ij}}{a_{ii}} \quad (i \neq j), \quad B_{ii} = 0 \\
            c_i &= \frac{b_i}{a_{ii}}
        \end{align*}
    \item Итерационная формула для $k$-го приближения:
        \[ x_i^{(k+1)} = \frac{1}{a_{ii}} \left(b_i - \sum_{j \neq i} a_{ij} x_j^{(k)}\right) \]
    \item Процесс повторяется до достижения заданной точности $\|x^{(k+1)} - x^{(k)}\| < \varepsilon$
\end{enumerate}

\subsection*{Метод Зейделя}

\textbf{Тип метода:} итерационный.

\textbf{Условия применимости:}
\begin{enumerate}
    \item Матрица $A$ должна иметь ненулевые диагональные элементы
    \item Достаточное условие сходимости: $A$ строго диагонально доминирующая
    \item Или достаточное условие: $A$ симметричная и положительно определённая
\end{enumerate}

\textbf{Суть метода:}
\begin{enumerate}
    \item Представление $A = L + D + U$, где $L$ — нижняя треугольная, $D$ — диагональная, $U$ — верхняя треугольная матрицы
    \item Итерационная формула: $(L + D)x^{(k+1)} = b - Ux^{(k)}$
    \item Итерационная формула для $k$-го приближения:
        \[ x_i^{(k+1)} = \frac{1}{a_{ii}}\left(b_i - \sum_{j=1}^{i-1}a_{ij}x_j^{(k+1)} - \sum_{j=i+1}^{n}a_{ij}x_j^{(k)}\right) \]
    \item Новые значения используются в последующих вычислениях текущей итерации
    \item Процесс повторяется до достижения заданной точности $\|x^{(k+1)} - x^{(k)}\| < \varepsilon$
\end{enumerate}

\subsection*{Метод верхней релаксации (SOR)}

\textbf{Тип метода:} итерационный.

\textbf{Условия применимости:}
\begin{enumerate}
    \item Матрица $A$ должна быть квадратной и невырожденной
    \item Для гарантии сходимости требуется строгое диагональное преобладание матрицы $A$ или её положительная определённость
    \item Параметр релаксации $\omega \in (0, 2)$. При $\omega = 1$ метод совпадает с методом Гаусса—Зейделя
\end{enumerate}

\textbf{Суть метода:}
Модификация метода Гаусса—Зейделя, где новое значение переменной вычисляется как взвешенная комбинация старого значения и значения, полученного на этапе релаксации.

\textbf{Итерационная формула:}
\[ x_i^{(k+1)} = (1 - \omega) x_i^{(k)} + \frac{\omega}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)} \right) \]

Параметр $\omega$ ускоряет сходимость: при $\omega > 1$ (верхняя релаксация) происходит ускорение, при $\omega < 1$ — замедление.

\subsection*{Метод градиентного спуска}

\textbf{Тип метода:} итерационный оптимизационный метод.

\textbf{Условия применимости:}
\begin{enumerate}
    \item Матрица $A$ должна быть симметричной и положительно определённой
\end{enumerate}

\textbf{Суть метода:}
Метод минимизирует квадратичную функцию:
\[ f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T A \mathbf{x} - \mathbf{b}^T \mathbf{x} \]
Градиент функции: $\nabla f(\mathbf{x}) = A\mathbf{x} - \mathbf{b}$

\textbf{Алгоритм:}
\begin{enumerate}
    \item Выбираем начальное приближение $\mathbf{x}_0$
    \item На каждой итерации $k$:
    \begin{itemize}
        \item Вычисляем невязку: $\mathbf{r}_k = A\mathbf{x}_k - \mathbf{b}$
        \item Вычисляем шаг: $\alpha_k = \frac{\mathbf{r}_k^T \mathbf{r}_k}{\mathbf{r}_k^T A \mathbf{r}_k}$
        \item Обновляем решение: $\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha_k \mathbf{r}_k$
    \end{itemize}
    \item Повторяем до достижения точности $\|\mathbf{r}_k\| < \varepsilon$
\end{enumerate}

\subsection*{Метод минимальных невязок}

\textbf{Тип метода:} итерационный.

\textbf{Условия применимости:}
\begin{enumerate}
    \item Матрица $A$ должна быть симметричной ($A = A^T$)
    \item Матрица $A$ должна быть положительно определённой ($x^T A x > 0$ для всех $x \neq 0$)
    \item Система должна быть совместной и определённой
\end{enumerate}

\textbf{Суть метода:}
Метод минимизирует норму невязки $r_k = b - A x_k$ на каждой итерации.

\textbf{Алгоритм:}
\begin{enumerate}
    \item Вычисляется невязка $r_k = b - A x_k$
    \item Находится итерационный параметр $\tau_k = \frac{(r_k, A r_k)}{(A r_k, A r_k)}$
    \item Новое приближение: $x_{k+1} = x_k + \tau_k r_k$
    \item Процесс повторяется до достижения заданной точности $\|r_k\| < \varepsilon$
\end{enumerate}

\subsection*{Метод сопряжённых градиентов (CG)}

\textbf{Тип метода:} итерационный оптимизационный метод.

\textbf{Условия применимости:}
\begin{enumerate}
    \item Матрица $A$ должна быть симметричной ($A = A^T$)
    \item Матрица $A$ должна быть положительно определённой ($x^T A x > 0$ для всех $x \ne 0$)
\end{enumerate}

\textbf{Суть метода:}
Метод минимизирует функционал невязки $f(x) = \frac{1}{2} x^T A x - b^T x$ в направлениях, взаимно сопряжённых относительно $A$.

\textbf{Алгоритм:}
\begin{enumerate}
    \item Инициализация: $x_0$, $r_0 = b - Ax_0$, $p_0 = r_0$
    \item Для $k = 0, 1, \dots$ до сходимости:
        \begin{align*}
            \alpha_k &= \frac{r_k^T r_k}{p_k^T A p_k} \\
            x_{k+1} &= x_k + \alpha_k p_k \\
            r_{k+1} &= r_k - \alpha_k A p_k \\
            \beta_k &= \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k} \\
            p_{k+1} &= r_{k+1} + \beta_k p_k
        \end{align*}
    Здесь $r_k$ — невязка на шаге $k$, $p_k$ — направление спуска
\end{enumerate}

\subsection*{Стабилизированный метод бисопряжённых градиентов (BiCGSTAB)}

\textbf{Тип метода:} итерационный метод для решения СЛАУ. Относится к классу проекционных методов подпространства Крылова.

\textbf{Условия применимости:}
\begin{enumerate}
    \item Матрица $A$ должна быть квадратной и невырожденной
    \item Для сходимости требуется, чтобы система была разрешимой и хорошо обусловленной
\end{enumerate}

\textbf{Суть метода:}
BiCGSTAB является улучшенной версией метода бисопряжённых градиентов (BiCG), где для стабилизации процесса сходимости используется дополнительное чередование с шагом метода минимальных невязок (GMRES).

\textbf{Алгоритм:}
\begin{enumerate}
    \item Инициализация: $r_0 = b - Ax_0$, $p_0 = r_0$, $\rho_0 = \alpha = \omega_0 = 1$, $v_0 = 0$
    \item Для $k = 1, 2, \dots$ до сходимости:
        \begin{align*}
            \rho_k &= (r_{k-1}, r_0^*) \\
            \beta &= (\rho_k / \rho_{k-1}) \times (\alpha / \omega_{k-1}) \\
            p_k &= r_{k-1} + \beta \times (p_{k-1} - \omega_{k-1} v_{k-1}) \\
            v_k &= A p_k \\
            \alpha &= \rho_k / (v_k, r_0^*) \\
            s &= r_{k-1} - \alpha v_k \\
            t &= A s \\
            \omega_k &= (t, s) / (t, t) \\
            x_k &= x_{k-1} + \alpha p_k + \omega_k s \\
            r_k &= s - \omega_k t
        \end{align*}
\end{enumerate}

\section{Исследование}

По описанной методологии проведено исследование. Все методы, кроме градиентного спуска, справились с СЛАУ. Зависимость нормы невязки от номера итерации приведена на рис. \ref{graf}. 

Метод верхней релаксации использовался с $\omega=1.1$.

 \begin{figure}[H]
 	\centering
 	\includegraphics[width=1\linewidth]{images/graf_residuals.png}
 	\caption{}
 	\label{graf}
 \end{figure}

\section{Обсуждение результатов}

Как уже было сказано, матрица системы \eqref{eq:sys} не является симметричной. Это нарушает условия применимости метода сопряженных градиентов, метода минимальных невязок и градиентного спуска. Несмотря на это, только последний метод не сошёлся, а первые два пришли к решение за незначительно отличающееся от других методов число итераций. 

Самая медленная сходимость оказалась у метода Якоби, если оценивать по количеству потребовавшихся итераций. Стоит заметить, что для более справедливой оценки скорости методов, требуется брать во внимание вычислительную сложность каждой итерации, что не рассматривается в данной работе.

Метод стабилизированных бисопряженных градиентов, самый современный из рассматриваемых, сошёлся всего за 4 итерации. Однако, при уменьшении $\epsilon$ до $10^{-10}$, он не сошёлся из-за $(t, t) = 0$. Это может быть вызвано тем, что все коэффициенты матрицы системы меньше 1, что приводит к вычислениям в области малых чисел.

Метод верхней релаксации, хоть и должен в теории при $\omega > 1$ ускорять сходимость по сравнению с методом Зейделя ($\omega =1 $), на самом деле сошёлся за большее число итераций и с меньшей точностью.

\section{Вывод}

Получена зависимость норм невязок от номера итерации для самых распространненых методов решения СЛАУ. На примере метода стабилизированных бисопряженных градиентов выяснено, что сходимость зависит от требуемой точности; кроме того, для нескольких методов на практике подтверждено, что достаточное условие сходимости не является необходимым.

\end{document}
